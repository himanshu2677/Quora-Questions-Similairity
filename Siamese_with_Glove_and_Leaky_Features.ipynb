{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Improvements brought in this notebook:-\n",
        "1.   Used Leaky Features based on questions popularity in dataset\n"
      ],
      "metadata": {
        "id": "Gl_KI19psZKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "CHz3fK6DtFwO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijiff1YOFM-K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOetmpQJzXCT",
        "outputId": "1aebe428-e6a8-42c6-bf34-8e71933415b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow  as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "BATCH_SIZE = 1536\n",
        "MAX_LENGTH = 64\n",
        "VOCAB_SIZE = 200000\n",
        "D_MODEL = 300\n"
      ],
      "metadata": {
        "id": "O69ysMD3iszl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Data"
      ],
      "metadata": {
        "id": "qRF4yK-BhQeT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sUz-ZAIHr6A"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/train.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5WzxKGiKUdJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9jzBGPyJe4q"
      },
      "outputs": [],
      "source": [
        "X_train, X_val = train_test_split(data,test_size=0.2,random_state=99)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLHahzbimLxl",
        "outputId": "4886f76f-13b8-457a-e789-214ca697ef28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "test_data = pd.read_csv('/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/test.csv')\n",
        "test_data = test_data[test_data['test_id']!='life in dublin?\"'].copy()\n",
        "test_data['test_id'] = test_data['test_id'].map(int)\n",
        "test_data = test_data.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk_VCR_FRoNd",
        "outputId": "a4c0facd-868c-475e-80ea-05e5c8a40b95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(370166, 34124)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(X_train),len(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1tEBokk4h36O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Text"
      ],
      "metadata": {
        "id": "EzsYr9y7h4hP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcuQVIzjKMzG",
        "outputId": "9110ddf5-1e2b-4a89-aa95-67f754105a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english')) - set(['not','what','why','how','who','whom','which'])\n",
        "stemmer = SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6jevNtWKRtv"
      },
      "outputs": [],
      "source": [
        "def remove_html_tags(text):\n",
        "  text = re.sub(r'<.*?>','',text)\n",
        "  return text\n",
        "\n",
        "def remove_special_characters(text):\n",
        "  text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "  text = re.sub(r\"what's\", \"what is \", text)\n",
        "  text = re.sub(r\"\\'s\", \" \", text)\n",
        "  text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "  text = re.sub(r\"can't\", \"cannot \", text)\n",
        "  text = re.sub(r\"n't\", \" not \", text)\n",
        "  text = re.sub(r\"i'm\", \"i am \", text)\n",
        "  text = re.sub(r\"\\'re\", \" are \", text)\n",
        "  text = re.sub(r\"\\'d\", \" would \", text)\n",
        "  text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "  text = re.sub(r\",\", \" \", text)\n",
        "  text = re.sub(r\"\\.\", \" \", text)\n",
        "  text = re.sub(r\"!\", \" ! \", text)\n",
        "  text = re.sub(r\"\\/\", \" \", text)\n",
        "  text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "  text = re.sub(r\"\\+\", \" + \", text)\n",
        "  text = re.sub(r\"\\-\", \" - \", text)\n",
        "  text = re.sub(r\"\\=\", \" = \", text)\n",
        "  text = re.sub(r\"'\", \" \", text)\n",
        "  text = re.sub(r\":\", \" : \", text)\n",
        "  text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "  text = re.sub(r\" e g \", \" eg \", text)\n",
        "  text = re.sub(r\" b g \", \" bg \", text)\n",
        "  text = re.sub(r\" u s \", \" american \", text)\n",
        "  text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "  text = re.sub(r\"e - mail\", \"email\", text)\n",
        "  text = re.sub(r\"j k\", \"jk\", text)\n",
        "  text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "  return text\n",
        "\n",
        "def lower_the_text(text):\n",
        "  return text.lower()\n",
        "\n",
        "def tokenize_text(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "def remove_stopwords(tokenized_text):\n",
        "  return [word for word in tokenized_text if word not in stop_words]\n",
        "\n",
        "def stem_text(tokenized_text):\n",
        "  return [stemmer.stem(word) for word in tokenized_text]\n",
        "\n",
        "\n",
        "def clean_text(text,tokenize_text_flag = False,rem_stopwords_flag = False, stem_text_flag = False,return_string = True):\n",
        "  text = remove_html_tags(text)\n",
        "  text = remove_special_characters(text)\n",
        "  text = lower_the_text(text)\n",
        "  \n",
        "  if(tokenize_text_flag):\n",
        "    text = tokenize_text(text)\n",
        "\n",
        "    if rem_stopwords_flag:\n",
        "      text = remove_stopwords(text)\n",
        "    if stem_text_flag:\n",
        "      text = stem_text(text)\n",
        "    \n",
        "    if(return_string):\n",
        "      return \" \".join(text)\n",
        "\n",
        "  return text\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C4dv572Ph_eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Data Generators for training"
      ],
      "metadata": {
        "id": "8PA1L7eciB6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEAKY_FEATURES = True\n",
        "\n",
        "q_dict = defaultdict(set)\n",
        "if LEAKY_FEATURES:\n",
        "  from collections import defaultdict\n",
        "  questions = pd.concat([X_train[['question1', 'question2']], \\\n",
        "          X_val[['question1', 'question2']],test_data[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
        "  \n",
        "  for i in range(questions.shape[0]):\n",
        "          q_dict[questions.question1[i]].add(questions.question2[i])\n",
        "          q_dict[questions.question2[i]].add(questions.question1[i])\n"
      ],
      "metadata": {
        "id": "Rl01sceG1ket"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YZKwgCaDw7M"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def ques_pair_generator_function(questions_list1,questions_list2,y_list = None,shuffle = False,clean_ques = True,clean_text = clean_text,leaky_features_flag = LEAKY_FEATURES,q_dict = q_dict):\n",
        "  def ques_pair_generator():\n",
        "    n_ques = len(questions_list1)\n",
        "    index_list = [i for i in range(n_ques)]\n",
        "    if(shuffle == True):\n",
        "      random.shuffle(index_list)\n",
        "    i = -1\n",
        "    while True:\n",
        "      i = i + 1\n",
        "      if(i == n_ques):\n",
        "        i = 0\n",
        "        if(shuffle == True):\n",
        "          random.shuffle(index_list)\n",
        "\n",
        "      q1 = str(questions_list1[index_list[i]])\n",
        "      q2 = str(questions_list2[index_list[i]])\n",
        "\n",
        "      if(leaky_features_flag):\n",
        "        count1 = len(q_dict[q1])*1.0\n",
        "        count2 = len(q_dict[q2])*1.0\n",
        "        count_common = len(q_dict[q1]&q_dict[q2])*1.0\n",
        "\n",
        "      if(clean_ques):\n",
        "        q1 = clean_text(q1)\n",
        "        q2 = clean_text(q2)\n",
        "        \n",
        "\n",
        "      if(y_list is not None):\n",
        "        y = y_list[index_list[i]]\n",
        "\n",
        "      if(y_list is None):\n",
        "        yield q1,q2,np.array([len(q1)/1.0,len(q2)/1.0,len(set(q1.split())&set(q2.split()))/1.0])\n",
        "      else:\n",
        "        yield q1,q2,np.array([len(q1)/1.0,len(q2)/1.0,len(set(q1.split())&set(q2.split()))/1.0]),y\n",
        "\n",
        "  return ques_pair_generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w62GrSj1SbOU"
      },
      "outputs": [],
      "source": [
        "train_generator = ques_pair_generator_function(X_train['question1'].to_list(),X_train['question2'].to_list(),X_train['is_duplicate'].to_list(),shuffle = True)\n",
        "val_generator = ques_pair_generator_function(X_val['question1'].to_list(),X_val['question2'].to_list(),X_val['is_duplicate'].to_list(),shuffle = False)\n",
        "test_generator = ques_pair_generator_function(test_data['question1'].to_list(),test_data['question2'].to_list(),shuffle = False)\n",
        "\n",
        "all_generator = ques_pair_generator_function(test_data['question1'].to_list()+X_val['question1'].to_list()+X_train['question1'].to_list(),test_data['question2'].to_list()+X_val['question2'].to_list()+X_train['question2'].to_list(),shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_size = X_train.shape[0]\n",
        "val_size = X_val.shape[0]\n",
        "test_size = test_data.shape[0]\n"
      ],
      "metadata": {
        "id": "FGk1tuopirF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dFsSTzxijCFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensorflow generators"
      ],
      "metadata": {
        "id": "nlKFEe4HjC13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums = 3 + LEAKY_FEATURES*3"
      ],
      "metadata": {
        "id": "jXJ2h7jz2Syh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPrCYnpNHT08"
      },
      "outputs": [],
      "source": [
        "raw_train_dataset = tf.data.Dataset.from_generator(train_generator,output_signature = (tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape=(nums,), dtype=tf.float32),tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "raw_train_dataset = raw_train_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "raw_val_dataset = tf.data.Dataset.from_generator(val_generator,output_signature = (tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape=(nums,), dtype=tf.float32),tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "raw_val_dataset = raw_val_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "raw_test_dataset = tf.data.Dataset.from_generator(test_generator,output_signature = (tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape=(nums,), dtype=tf.float32)))\n",
        "raw_test_dataset = raw_test_dataset.map(lambda q1,q2,lengths: (q1,q2,lengths,-1))\n",
        "raw_test_dataset = raw_test_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "raw_all_dataset = tf.data.Dataset.from_generator(all_generator,output_signature = (tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape=(nums,), dtype=tf.float32)))\n",
        "raw_all_dataset = raw_all_dataset.map(lambda q1,q2,lengths: (q1,q2,lengths,-1))\n",
        "raw_all_dataset = raw_all_dataset.batch(BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9oxePv8ji_-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vectorizer Layer"
      ],
      "metadata": {
        "id": "M9-CORETjJ2Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WuE-Yt9JS8H"
      },
      "outputs": [],
      "source": [
        "vectorize_layer = layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_LENGTH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRjKUpTKKcui"
      },
      "outputs": [],
      "source": [
        "all = raw_all_dataset.map(lambda q1,q2,l,y: tf.concat([q1,q2],axis = 0)).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4CYKzq-K87W"
      },
      "outputs": [],
      "source": [
        "# vectorize_layer.adapt(all,steps = (train_size + val_size + test_size)//BATCH_SIZE + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-bljxjQR7OU"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# pickle.dump({'config': vectorize_layer.get_config(),\n",
        "#              'weights': vectorize_layer.get_weights()}\n",
        "#             , open(\"/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/vectorize_layer.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZDkVm7Tio8K"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from_disk = pickle.load(open(\"/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/vectorize_layer.pkl\", \"rb\"))\n",
        "vectorize_layer = layers.TextVectorization.from_config(from_disk['config'])\n",
        "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
        "vectorize_layer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "vectorize_layer.set_weights(from_disk['weights'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEf9MQehOy2c",
        "outputId": "7102f922-43b4-48e3-afa3-afa7d62acf59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120569"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len(vectorize_layer.get_vocabulary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwpsbr6fKb-4"
      },
      "outputs": [],
      "source": [
        "def vectorize_ques(q1,q2,lengths,label):\n",
        "  return (vectorize_layer(q1), vectorize_layer(q2),lengths),label\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "56fyXWTBjQfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensorflow Datasets"
      ],
      "metadata": {
        "id": "rFIMycXojRXw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h44RZLxsfcl3"
      },
      "outputs": [],
      "source": [
        "train_dataset = raw_train_dataset.map(vectorize_ques).cache().prefetch(buffer_size = 1)\n",
        "val_dataset = raw_val_dataset.map(vectorize_ques).cache().prefetch(buffer_size = 1)\n",
        "test_dataset = raw_test_dataset.map(vectorize_ques).cache().prefetch(buffer_size = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Temp"
      ],
      "metadata": {
        "id": "uehxJEO9jW2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_siamese_network(vocab_size = VOCAB_SIZE,d_model = D_MODEL,dropout_rate = 0.20,batch_size = BATCH_SIZE,max_length = MAX_LENGTH): \n",
        "\n",
        "  def build_tf_lstm_model():\n",
        "    encoded_question = layers.Input(batch_shape = (batch_size,max_length),name = 'input_encoded_question')\n",
        "    embeddings = layers.Embedding(vocab_size,d_model,input_length=max_length,name = 'embedding_layer')(encoded_question)\n",
        "    layer_1 =  layers.Bidirectional(layers.LSTM(d_model,activation = 'tanh',return_sequences=True,dropout = dropout_rate,stateful  = True,name = 'lstm_1'),name = 'bidirectional_1')(embeddings)\n",
        "    layer_2 =  layers.Bidirectional(layers.LSTM(d_model,return_sequences=False,stateful  = True,name = 'lstm_2'),name = 'bidirectional_2')(layer_1)\n",
        "    lstm_model  = Model(inputs = [encoded_question],outputs = [layer_2],name = 'LSTM')\n",
        "\n",
        "    return lstm_model\n",
        "\n",
        "\n",
        "  \n",
        "  encoded_question1 = layers.Input(batch_shape = (batch_size,max_length),name = 'input_encoded_question1')\n",
        "  encoded_question2 = layers.Input(batch_shape = (batch_size,max_length),name = 'input_encoded_question2')\n",
        "  \n",
        "  \n",
        "  \n",
        "  lstm_model = build_tf_lstm_model()\n",
        "  \n",
        "  embedded_questions1 = lstm_model(encoded_question1)\n",
        "  embedded_questions2 = lstm_model(encoded_question2)\n",
        "\n",
        "  lengths = layers.Input(batch_shape = (batch_size,3),name = 'input_lengths')\n",
        "  lengths = layers.BatchNormalization(name = 'batch_norm_lengths')(lengths)\n",
        "\n",
        "  lengths_hidden = layers.Dense(d_model//2,name = 'lengths_hidden')(lengths)\n",
        "  lengths_hidden = layers.BatchNormalization()(lengths_hidden)\n",
        "  lengths_hidden = layers.Activation('relu')(lengths_hidden)\n",
        "  lengths_hidden = layers.Dropout(dropout_rate)(lengths_hidden)\n",
        "\n",
        "  lengths_hidden = layers.Dense(d_model,name = 'lengths_hidden_2')(lengths_hidden)\n",
        "\n",
        "  concatenated = tf.concat([embedded_questions1,embedded_questions2,lengths_hidden],axis = -1)\n",
        "  concatenated = layers.BatchNormalization()(concatenated)\n",
        "  concatenated = layers.Dropout(0.20)(concatenated)\n",
        "\n",
        "  concatenated_dense = layers.Dense(d_model,name = 'concatenated_dense')(concatenated)\n",
        "  concatenated_dense = layers.BatchNormalization()(concatenated_dense)\n",
        "  concatenated_dense = layers.Activation('relu')(concatenated_dense)\n",
        "  concatenated_dense = layers.Dropout(0.20)(concatenated_dense)\n",
        "\n",
        "  concatenated_dense = layers.Dense(d_model//2,name = 'concatenated_dense_2')(concatenated_dense)\n",
        "  concatenated_dense = layers.BatchNormalization()(concatenated_dense)\n",
        "  concatenated_dense = layers.Activation('relu')(concatenated_dense)\n",
        "  concatenated_dense = layers.Dropout(0.20)(concatenated_dense)\n",
        "\n",
        "  \n",
        "  output = layers.Dense(1,name = 'output_logit')(concatenated_dense)\n",
        "\n",
        "  NN_model = Model(inputs = [embedded_questions1,embedded_questions2,lengths_hidden],outputs = [output])\n",
        "\n",
        "  output1 = NN_model((embedded_questions1,embedded_questions2,lengths_hidden))\n",
        "  output2 = NN_model((embedded_questions2,embedded_questions1,lengths_hidden))\n",
        "\n",
        "  output_logit = (output1 + output2)/2\n",
        "\n",
        "  model  = Model(inputs = [encoded_question1,encoded_question2,lengths],outputs = [output_logit])\n",
        "  \n",
        "  return model\n"
      ],
      "metadata": {
        "id": "RUyt2urJ2vlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = BATCH_SIZE\n",
        "siamese_model = build_siamese_network()\n",
        "\n",
        "siamese_model.compile(loss  =  tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
        "                optimizer = Adam(learning_rate  = 5e-3),\n",
        "                metrics = [tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.BinaryIoU()])\n",
        "\n",
        "early_stopping = EarlyStopping(min_delta = 0.01,patience = 3,restore_best_weights=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "NMK62zRp2zQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.summary()"
      ],
      "metadata": {
        "id": "JJa6IU_t21WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = siamese_model.fit(train_dataset,\n",
        "                            steps_per_epoch = train_size//batch_size + 1,\n",
        "                            epochs = 20,\n",
        "                            validation_data=val_dataset,\n",
        "                            validation_steps = val_size//batch_size + 1,\n",
        "                            callbacks = [early_stopping])\n"
      ],
      "metadata": {
        "id": "3amv4IWM26yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.save('/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/siamese_with_features.h5')\n"
      ],
      "metadata": {
        "id": "nrV-IjCk29wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "PX7IKLNEjXjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv99YA0GVYFh"
      },
      "outputs": [],
      "source": [
        "# voc = vectorize_layer.get_vocabulary()\n",
        "# word_index = dict(zip(voc, range(len(voc))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEebLd_tRLzl"
      },
      "outputs": [],
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.840B.300d.zip -P \"/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/\"\n",
        "# !unzip -q \"/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/glove.840B.300d.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us_pkV8GRM0u"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# path_to_glove_file = os.path.join(\"/content/glove.840B.300d.txt\")\n",
        "\n",
        "# embeddings_index = {}\n",
        "# f = open(path_to_glove_file, encoding='utf-8')\n",
        "\n",
        "# # for line in tqdm(f):\n",
        "# for line in f:\n",
        "#     values = line.split()\n",
        "#     # word = values[0]\n",
        "#     word = (''.join(values[:-300])).lower()   \n",
        "#     # coefs = np.asarray(values[1:], dtype='float32')\n",
        "#     coefs = np.asarray(values[-300:], dtype='float32')\n",
        "#     if(word in embeddings_index):\n",
        "#       embeddings_index[word].append(coefs)\n",
        "#     else:\n",
        "#       embeddings_index[word] = [coefs]\n",
        "\n",
        "\n",
        "# f.close()\n",
        "\n",
        "# print(\"Found %s word vectors.\" % len(embeddings_index))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRkOyi6qvHJZ"
      },
      "outputs": [],
      "source": [
        "# for word in embeddings_index:\n",
        "#   embeddings_index[word] = np.mean(embeddings_index[word],axis = 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCRo6TlJRMb9"
      },
      "outputs": [],
      "source": [
        "# num_tokens = len(voc) + 2\n",
        "# embedding_dim = 300\n",
        "# hits = 0\n",
        "# misses = 0\n",
        "# missed_words = []\n",
        "# # Prepare embedding matrix\n",
        "# embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # Words not found in embedding index will be all-zeros.\n",
        "#         # This includes the representation for \"padding\" and \"OOV\"\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "#         hits += 1\n",
        "#     else:\n",
        "#         missed_words.append(word)\n",
        "#         misses += 1\n",
        "# print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating embedding layer"
      ],
      "metadata": {
        "id": "hRaFtxcUudEY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1qQ8YSPiEDC"
      },
      "outputs": [],
      "source": [
        "# embedding_layer = layers.Embedding(\n",
        "#     num_tokens,\n",
        "#     embedding_dim,\n",
        "#     embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "#     trainable=False,\n",
        "#     name = 'embedding_layer_300'\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M77v0vMwxXip"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# pickle.dump({'config': embedding_layer.get_config(),\n",
        "#              'weights': embedding_layer.get_weights()}\n",
        "#             , open(\"/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/embedding_layer.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxQb4vdsxs5r"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from_disk = pickle.load(open(\"/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/embedding_layer.pkl\", \"rb\"))\n",
        "embedding_layer = layers.Embedding.from_config(from_disk['config'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76tMAW2diU5-"
      },
      "outputs": [],
      "source": [
        "def build_siamese_network_glove(vocab_size = VOCAB_SIZE,d_model = D_MODEL,dropout_rate = 0.20,batch_size = BATCH_SIZE,max_length = MAX_LENGTH): \n",
        "\n",
        "  def build_tf_lstm_model_glove():\n",
        "    encoded_question = layers.Input(batch_shape = (batch_size,max_length),name = 'input_encoded_question')\n",
        "    embeddings = embedding_layer(encoded_question)\n",
        "    layer_1 =  layers.Bidirectional(layers.LSTM(d_model,activation = 'tanh',return_sequences=True,dropout = dropout_rate,stateful  = True,name = 'lstm_1'),name = 'bidirectional_1')(embeddings)\n",
        "    layer_2 =  layers.Bidirectional(layers.LSTM(d_model,return_sequences=False,stateful  = True,name = 'lstm_2'),name = 'bidirectional_2')(layer_1)\n",
        "    lstm_model  = Model(inputs = [encoded_question],outputs = [layer_2],name = 'LSTM')\n",
        "\n",
        "    return lstm_model\n",
        "\n",
        "\n",
        "  \n",
        "  encoded_question1 = layers.Input(batch_shape = (batch_size,max_length),name = 'input_encoded_question1')\n",
        "  encoded_question2 = layers.Input(batch_shape = (batch_size,max_length),name = 'input_encoded_question2')\n",
        "  \n",
        "  \n",
        "  \n",
        "  lstm_model = build_tf_lstm_model_glove()\n",
        "  \n",
        "  embedded_questions1 = lstm_model(encoded_question1)\n",
        "  embedded_questions2 = lstm_model(encoded_question2)\n",
        "\n",
        "  lengths = layers.Input(batch_shape = (batch_size,3),name = 'input_lengths')\n",
        "  lengths = layers.BatchNormalization(name = 'batch_norm_lengths')(lengths)\n",
        "\n",
        "  lengths_hidden = layers.Dense(d_model//2,name = 'lengths_hidden1')(lengths)\n",
        "  lengths_hidden = layers.BatchNormalization()(lengths_hidden)\n",
        "  lengths_hidden = layers.Activation('relu')(lengths_hidden)\n",
        "  lengths_hidden = layers.Dropout(dropout_rate)(lengths_hidden)\n",
        "\n",
        "  lengths_hidden = layers.Dense(d_model,name = 'lengths_hidden2')(lengths_hidden)\n",
        "\n",
        "  concatenated = tf.concat([embedded_questions1,embedded_questions2,lengths_hidden],axis = -1)\n",
        "  concatenated = layers.BatchNormalization()(concatenated)\n",
        "  concatenated = layers.Activation('relu')(concatenated)\n",
        "  concatenated = layers.Dropout(0.20)(concatenated)\n",
        "\n",
        "  concatenated_dense = layers.Dense(d_model,name = 'concatenated_dense1')(concatenated)\n",
        "  concatenated_dense = layers.BatchNormalization()(concatenated_dense)\n",
        "  concatenated_dense = layers.Activation('relu')(concatenated_dense)\n",
        "  concatenated_dense = layers.Dropout(0.20)(concatenated_dense)\n",
        "\n",
        "  concatenated_dense = layers.Dense(d_model//2,name = 'concatenated_dense2')(concatenated_dense)\n",
        "  concatenated_dense = layers.BatchNormalization()(concatenated_dense)\n",
        "  concatenated_dense = layers.Activation('relu')(concatenated_dense)\n",
        "  concatenated_dense = layers.Dropout(0.20)(concatenated_dense)\n",
        "\n",
        "  \n",
        "  output = layers.Dense(1,name = 'output_logit')(concatenated_dense)\n",
        "\n",
        "  NN_model = Model(inputs = [embedded_questions1,embedded_questions2,lengths_hidden],outputs = [output])\n",
        "\n",
        "  output1 = NN_model((embedded_questions1,embedded_questions2,lengths_hidden))\n",
        "  output2 = NN_model((embedded_questions2,embedded_questions1,lengths_hidden))\n",
        "\n",
        "  output_logit = (output1 + output2)/2\n",
        "\n",
        "  model  = Model(inputs = [encoded_question1,encoded_question2,lengths],outputs = [output_logit])\n",
        "  \n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0gEHK7UiqWt"
      },
      "outputs": [],
      "source": [
        "batch_size = BATCH_SIZE\n",
        "siamese_model = build_siamese_network_glove(batch_size = batch_size)\n",
        "\n",
        "siamese_model.compile(loss  =  tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
        "                optimizer='nadam',\n",
        "                metrics = [tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.BinaryIoU()])\n",
        "\n",
        "early_stopping = EarlyStopping(min_delta = 0.01,patience = 5,restore_best_weights=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HHcVQIZ9yeCS"
      },
      "outputs": [],
      "source": [
        "history = siamese_model.fit(train_dataset,\n",
        "                            batch_size = BATCH_SIZE,\n",
        "                            steps_per_epoch = train_size//batch_size + 1,\n",
        "                            epochs = 20,\n",
        "                            validation_data=val_dataset,\n",
        "                            validation_steps = val_size//batch_size + 1,\n",
        "                            callbacks = [early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYusv8P2zPZs"
      },
      "outputs": [],
      "source": [
        "siamese_model.save('/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/siamese_with_features_glove_and_leaky.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log-loss on Kaggle test set:- Private: 0.17376 Public: 0.16979\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YyloRuAnvfQF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWgFM_UJiEhk"
      },
      "source": [
        "### Prediction on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwFDSHVTw9Dv"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024\n",
        "siamese_model = build_siamese_network_glove(batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSWtm63Sw9Dw"
      },
      "outputs": [],
      "source": [
        "siamese_model.load_weights('/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/siamese_with_features_glove_and_leaky.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61915d0f-7a90-4f72-b218-1a7c9ee33fa1",
        "id": "eJ53RTMVw9Dx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1528/1528 [==============================] - 738s 482ms/step\n"
          ]
        }
      ],
      "source": [
        "y_test_predict = siamese_model.predict(test_dataset,steps=test_data.shape[0]//batch_size + 1,verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLg2F9Wlw9Dy"
      },
      "outputs": [],
      "source": [
        "test_data['is_duplicate'] = y_test_predict[:test_data.shape[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5kaGgZGw9Dz"
      },
      "outputs": [],
      "source": [
        "test_data['is_duplicate'] = test_data['is_duplicate'].apply(lambda x : np.exp(x)/(np.exp(x) + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z7v2MSXw9Dz"
      },
      "outputs": [],
      "source": [
        "submission = test_data[['test_id','is_duplicate']].copy()\n",
        "submission.to_csv('/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/siamese_with_features_glove_and_leaky.csv',index = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b-ZFbnVrxJx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
