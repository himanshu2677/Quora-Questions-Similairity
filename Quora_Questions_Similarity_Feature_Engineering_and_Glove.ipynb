{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Improvements brought in this notebook:-\n",
        "1.   Added features like number of common words in question pair\n",
        "2.   Not stemming the words now\n",
        "3.   Using glove embeddings in the 2nd half of notebook\n"
      ],
      "metadata": {
        "id": "Gl_KI19psZKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "CHz3fK6DtFwO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ijiff1YOFM-K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOetmpQJzXCT",
        "outputId": "1aebe428-e6a8-42c6-bf34-8e71933415b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow  as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "BATCH_SIZE = 1536\n",
        "MAX_LENGTH = 64\n",
        "VOCAB_SIZE = 200000\n",
        "D_MODEL = 300\n"
      ],
      "metadata": {
        "id": "O69ysMD3iszl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Data"
      ],
      "metadata": {
        "id": "qRF4yK-BhQeT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4sUz-ZAIHr6A"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/train.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p5WzxKGiKUdJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g9jzBGPyJe4q"
      },
      "outputs": [],
      "source": [
        "X_train, X_val = train_test_split(data,test_size=0.2,random_state=99)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLHahzbimLxl",
        "outputId": "4886f76f-13b8-457a-e789-214ca697ef28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "test_data = pd.read_csv('/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/test.csv')\n",
        "test_data = test_data[test_data['test_id']!='life in dublin?\"'].copy()\n",
        "test_data['test_id'] = test_data['test_id'].map(int)\n",
        "test_data = test_data.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk_VCR_FRoNd",
        "outputId": "a4c0facd-868c-475e-80ea-05e5c8a40b95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(370166, 34124)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(X_train),len(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1tEBokk4h36O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Text"
      ],
      "metadata": {
        "id": "EzsYr9y7h4hP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcuQVIzjKMzG",
        "outputId": "9110ddf5-1e2b-4a89-aa95-67f754105a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english')) - set(['not','what','why','how','who','whom','which'])\n",
        "stemmer = SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "G6jevNtWKRtv"
      },
      "outputs": [],
      "source": [
        "def remove_html_tags(text):\n",
        "  text = re.sub(r'<.*?>','',text)\n",
        "  return text\n",
        "\n",
        "def remove_special_characters(text):\n",
        "  text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "  text = re.sub(r\"what's\", \"what is \", text)\n",
        "  text = re.sub(r\"\\'s\", \" \", text)\n",
        "  text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "  text = re.sub(r\"can't\", \"cannot \", text)\n",
        "  text = re.sub(r\"n't\", \" not \", text)\n",
        "  text = re.sub(r\"i'm\", \"i am \", text)\n",
        "  text = re.sub(r\"\\'re\", \" are \", text)\n",
        "  text = re.sub(r\"\\'d\", \" would \", text)\n",
        "  text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "  text = re.sub(r\",\", \" \", text)\n",
        "  text = re.sub(r\"\\.\", \" \", text)\n",
        "  text = re.sub(r\"!\", \" ! \", text)\n",
        "  text = re.sub(r\"\\/\", \" \", text)\n",
        "  text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "  text = re.sub(r\"\\+\", \" + \", text)\n",
        "  text = re.sub(r\"\\-\", \" - \", text)\n",
        "  text = re.sub(r\"\\=\", \" = \", text)\n",
        "  text = re.sub(r\"'\", \" \", text)\n",
        "  text = re.sub(r\":\", \" : \", text)\n",
        "  text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "  text = re.sub(r\" e g \", \" eg \", text)\n",
        "  text = re.sub(r\" b g \", \" bg \", text)\n",
        "  text = re.sub(r\" u s \", \" american \", text)\n",
        "  text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "  text = re.sub(r\"e - mail\", \"email\", text)\n",
        "  text = re.sub(r\"j k\", \"jk\", text)\n",
        "  text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "  return text\n",
        "\n",
        "def lower_the_text(text):\n",
        "  return text.lower()\n",
        "\n",
        "def tokenize_text(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "def remove_stopwords(tokenized_text):\n",
        "  return [word for word in tokenized_text if word not in stop_words]\n",
        "\n",
        "def stem_text(tokenized_text):\n",
        "  return [stemmer.stem(word) for word in tokenized_text]\n",
        "\n",
        "\n",
        "def clean_text(text,tokenize_text_flag = False,rem_stopwords_flag = False, stem_text_flag = False,return_string = True):\n",
        "  text = remove_html_tags(text)\n",
        "  text = remove_special_characters(text)\n",
        "  text = lower_the_text(text)\n",
        "  \n",
        "  if(tokenize_text_flag):\n",
        "    text = tokenize_text(text)\n",
        "\n",
        "    if rem_stopwords_flag:\n",
        "      text = remove_stopwords(text)\n",
        "    if stem_text_flag:\n",
        "      text = stem_text(text)\n",
        "    \n",
        "    if(return_string):\n",
        "      return \" \".join(text)\n",
        "\n",
        "  return text\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C4dv572Ph_eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Data Generators for training"
      ],
      "metadata": {
        "id": "8PA1L7eciB6G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5YZKwgCaDw7M"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def ques_pair_generator_function(questions_list1,questions_list2,y_list = None,shuffle = False,clean_ques = True,clean_text = clean_text):\n",
        "  def ques_pair_generator():\n",
        "    n_ques = len(questions_list1)\n",
        "    index_list = [i for i in range(n_ques)]\n",
        "    if(shuffle == True):\n",
        "      random.shuffle(index_list)\n",
        "    i = -1\n",
        "    while True:\n",
        "      i = i + 1\n",
        "      if(i == n_ques):\n",
        "        i = 0\n",
        "        if(shuffle == True):\n",
        "          random.shuffle(index_list)\n",
        "\n",
        "      q1 = str(questions_list1[index_list[i]])\n",
        "      q2 = str(questions_list2[index_list[i]])\n",
        "\n",
        "      if(clean_ques):\n",
        "        q1 = clean_text(q1)\n",
        "        q2 = clean_text(q2)\n",
        "        \n",
        "\n",
        "      if(y_list is not None):\n",
        "        y = y_list[index_list[i]]\n",
        "\n",
        "      if(y_list is None):\n",
        "        yield q1,q2,np.array([len(q1)/1.0,len(q2)/1.0,len(set(q1.split())&set(q2.split()))/1.0])\n",
        "      else:\n",
        "        yield q1,q2,np.array([len(q1)/1.0,len(q2)/1.0,len(set(q1.split())&set(q2.split()))/1.0]),y\n",
        "\n",
        "  return ques_pair_generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w62GrSj1SbOU"
      },
      "outputs": [],
      "source": [
        "train_generator = ques_pair_generator_function(X_train['question1'].to_list(),X_train['question2'].to_list(),X_train['is_duplicate'].to_list(),shuffle = True)\n",
        "val_generator = ques_pair_generator_function(X_val['question1'].to_list(),X_val['question2'].to_list(),X_val['is_duplicate'].to_list(),shuffle = False)\n",
        "test_generator = ques_pair_generator_function(test_data['question1'].to_list(),test_data['question2'].to_list(),shuffle = False)\n",
        "\n",
        "all_generator = ques_pair_generator_function(test_data['question1'].to_list()+X_val['question1'].to_list()+X_train['question1'].to_list(),test_data['question2'].to_list()+X_val['question2'].to_list()+X_train['question2'].to_list(),shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_size = X_train.shape[0]\n",
        "val_size = X_val.shape[0]\n",
        "test_size = test_data.shape[0]\n"
      ],
      "metadata": {
        "id": "FGk1tuopirF-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dFsSTzxijCFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensorflow generators"
      ],
      "metadata": {
        "id": "nlKFEe4HjC13"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BPrCYnpNHT08"
      },
      "outputs": [],
      "source": [
        "raw_train_dataset = tf.data.Dataset.from_generator(train_generator,output_signature = (tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape=(3,), dtype=tf.float32),tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "raw_train_dataset = raw_train_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "raw_val_dataset = tf.data.Dataset.from_generator(val_generator,output_signature = (tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape=(3,), dtype=tf.float32),tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "raw_val_dataset = raw_val_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "raw_test_dataset = tf.data.Dataset.from_generator(test_generator,output_signature = (tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape=(3,), dtype=tf.float32)))\n",
        "raw_test_dataset = raw_test_dataset.map(lambda q1,q2,lengths: (q1,q2,lengths,-1))\n",
        "raw_test_dataset = raw_test_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "raw_all_dataset = tf.data.Dataset.from_generator(all_generator,output_signature = (tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape = (), dtype = tf.string),tf.TensorSpec(shape=(3,), dtype=tf.float32)))\n",
        "raw_all_dataset = raw_all_dataset.map(lambda q1,q2,lengths: (q1,q2,lengths,-1))\n",
        "raw_all_dataset = raw_all_dataset.batch(BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9oxePv8ji_-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vectorizer Layer"
      ],
      "metadata": {
        "id": "M9-CORETjJ2Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0WuE-Yt9JS8H"
      },
      "outputs": [],
      "source": [
        "vectorize_layer = layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_LENGTH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FRjKUpTKKcui"
      },
      "outputs": [],
      "source": [
        "all = raw_all_dataset.map(lambda q1,q2,l,y: tf.concat([q1,q2],axis = 0)).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "O4CYKzq-K87W"
      },
      "outputs": [],
      "source": [
        "# vectorize_layer.adapt(all,steps = (train_size + val_size + test_size)//BATCH_SIZE + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "C-bljxjQR7OU"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# pickle.dump({'config': vectorize_layer.get_config(),\n",
        "#              'weights': vectorize_layer.get_weights()}\n",
        "#             , open(\"/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/vectorize_layer.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EZDkVm7Tio8K"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from_disk = pickle.load(open(\"/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/vectorize_layer.pkl\", \"rb\"))\n",
        "vectorize_layer = layers.TextVectorization.from_config(from_disk['config'])\n",
        "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
        "vectorize_layer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "vectorize_layer.set_weights(from_disk['weights'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEf9MQehOy2c",
        "outputId": "7102f922-43b4-48e3-afa3-afa7d62acf59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120569"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len(vectorize_layer.get_vocabulary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bwpsbr6fKb-4"
      },
      "outputs": [],
      "source": [
        "def vectorize_ques(q1,q2,lengths,label):\n",
        "  return (vectorize_layer(q1), vectorize_layer(q2),lengths),label\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "56fyXWTBjQfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensorflow Datasets"
      ],
      "metadata": {
        "id": "rFIMycXojRXw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "h44RZLxsfcl3"
      },
      "outputs": [],
      "source": [
        "train_dataset = raw_train_dataset.map(vectorize_ques).cache().prefetch(buffer_size = 1)\n",
        "val_dataset = raw_val_dataset.map(vectorize_ques).cache().prefetch(buffer_size = 1)\n",
        "test_dataset = raw_test_dataset.map(vectorize_ques).cache().prefetch(buffer_size = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uehxJEO9jW2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "PX7IKLNEjXjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Siamese model architecture"
      ],
      "metadata": {
        "id": "wU306E2QjlH9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHvt2vGjmoUq"
      },
      "outputs": [],
      "source": [
        "def build_siamese_network(vocab_size = VOCAB_SIZE,d_model = D_MODEL,dropout_rate = 0.20,batch_size = BATCH_SIZE,max_length = MAX_LENGTH): \n",
        "\n",
        "  def build_tf_lstm_model():\n",
        "    encoded_question = layers.Input(batch_shape = (batch_size,max_length),name = 'input_encoded_question')\n",
        "    embeddings = layers.Embedding(vocab_size,d_model,input_length=max_length,name = 'embedding_layer')(encoded_question)\n",
        "    layer_1 =  layers.Bidirectional(layers.LSTM(d_model,activation = 'tanh',return_sequences=True,dropout = dropout_rate,stateful  = True,name = 'lstm_1'),name = 'bidirectional_1')(embeddings)\n",
        "    layer_2 =  layers.Bidirectional(layers.LSTM(d_model,return_sequences=False,stateful  = True,name = 'lstm_2'),name = 'bidirectional_2')(layer_1)\n",
        "    lstm_model  = Model(inputs = [encoded_question],outputs = [layer_2],name = 'LSTM')\n",
        "\n",
        "    return lstm_model\n",
        "\n",
        "\n",
        "  \n",
        "  encoded_question1 = layers.Input(batch_shape = (batch_size,max_length),name = 'input_encoded_question1')\n",
        "  encoded_question2 = layers.Input(batch_shape = (batch_size,max_length),name = 'input_encoded_question2')\n",
        "  \n",
        "  \n",
        "  \n",
        "  lstm_model = build_tf_lstm_model()\n",
        "  \n",
        "  embedded_questions1 = lstm_model(encoded_question1)\n",
        "  embedded_questions2 = lstm_model(encoded_question2)\n",
        "\n",
        "  lengths = layers.Input(batch_shape = (batch_size,3),name = 'input_lengths')\n",
        "  lengths = layers.BatchNormalization(name = 'batch_norm_lengths')(lengths)\n",
        "\n",
        "  lengths_hidden = layers.Dense(d_model//2,name = 'lengths_hidden')(lengths)\n",
        "  lengths_hidden = layers.BatchNormalization()(lengths_hidden)\n",
        "  lengths_hidden = layers.Activation('relu')(lengths_hidden)\n",
        "  lengths_hidden = layers.Dropout(dropout_rate)(lengths_hidden)\n",
        "\n",
        "  lengths_hidden = layers.Dense(d_model,name = 'lengths_hidden_2')(lengths_hidden)\n",
        "\n",
        "  concatenated = tf.concat([embedded_questions1,embedded_questions2,lengths_hidden],axis = -1)\n",
        "  concatenated = layers.BatchNormalization()(concatenated)\n",
        "  concatenated = layers.Dropout(0.20)(concatenated)\n",
        "\n",
        "  concatenated_dense = layers.Dense(d_model,name = 'concatenated_dense')(concatenated)\n",
        "  concatenated_dense = layers.BatchNormalization()(concatenated_dense)\n",
        "  concatenated_dense = layers.Activation('relu')(concatenated_dense)\n",
        "  concatenated_dense = layers.Dropout(0.20)(concatenated_dense)\n",
        "\n",
        "  concatenated_dense = layers.Dense(d_model//2,name = 'concatenated_dense_2')(concatenated_dense)\n",
        "  concatenated_dense = layers.BatchNormalization()(concatenated_dense)\n",
        "  concatenated_dense = layers.Activation('relu')(concatenated_dense)\n",
        "  concatenated_dense = layers.Dropout(0.20)(concatenated_dense)\n",
        "\n",
        "  \n",
        "  output = layers.Dense(1,name = 'output_logit')(concatenated_dense)\n",
        "\n",
        "  NN_model = Model(inputs = [embedded_questions1,embedded_questions2,lengths_hidden],outputs = [output])\n",
        "\n",
        "  output1 = NN_model((embedded_questions1,embedded_questions2,lengths_hidden))\n",
        "  output2 = NN_model((embedded_questions2,embedded_questions1,lengths_hidden))\n",
        "\n",
        "  output_logit = (output1 + output2)/2\n",
        "\n",
        "  model  = Model(inputs = [encoded_question1,encoded_question2,lengths],outputs = [output_logit])\n",
        "  \n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czz71mV8dZfP"
      },
      "outputs": [],
      "source": [
        "batch_size = BATCH_SIZE\n",
        "siamese_model = build_siamese_network()\n",
        "\n",
        "siamese_model.compile(loss  =  tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
        "                optimizer = Adam(learning_rate  = 5e-3),\n",
        "                metrics = [tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.BinaryIoU()])\n",
        "\n",
        "early_stopping = EarlyStopping(min_delta = 0.01,patience = 3,restore_best_weights=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b4DKhrgwI9y",
        "outputId": "87bf3213-7518-44c6-c404-2da17de3d51e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(1536, 3)]          0           []                               \n",
            "                                                                                                  \n",
            " lengths_hidden (Dense)         (1536, 150)          600         ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (1536, 150)         600         ['lengths_hidden[1][0]']         \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (1536, 150)          0           ['batch_normalization[1][0]']    \n",
            "                                                                                                  \n",
            " input_encoded_question1 (Input  [(1536, 64)]        0           []                               \n",
            " Layer)                                                                                           \n",
            "                                                                                                  \n",
            " input_encoded_question2 (Input  [(1536, 64)]        0           []                               \n",
            " Layer)                                                                                           \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (1536, 150)          0           ['activation[1][0]']             \n",
            "                                                                                                  \n",
            " LSTM (Functional)              (1536, 600)          63604800    ['input_encoded_question1[0][0]',\n",
            "                                                                  'input_encoded_question2[0][0]']\n",
            "                                                                                                  \n",
            " lengths_hidden_2 (Dense)       (1536, 300)          45300       ['dropout[1][0]']                \n",
            "                                                                                                  \n",
            " model (Functional)             (1536, 1)            503401      ['LSTM[2][0]',                   \n",
            "                                                                  'LSTM[3][0]',                   \n",
            "                                                                  'lengths_hidden_2[1][0]',       \n",
            "                                                                  'LSTM[3][0]',                   \n",
            "                                                                  'LSTM[2][0]',                   \n",
            "                                                                  'lengths_hidden_2[1][0]']       \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (1536, 1)           0           ['model[2][0]',                  \n",
            " da)                                                              'model[3][0]']                  \n",
            "                                                                                                  \n",
            " tf.math.truediv (TFOpLambda)   (1536, 1)            0           ['tf.__operators__.add[1][0]']   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 64,154,701\n",
            "Trainable params: 64,150,501\n",
            "Non-trainable params: 4,200\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "siamese_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "slvEqbZkkEzu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhHPClsVpz1j",
        "outputId": "3db4ea31-ab6b-452b-a4a5-0596b1065533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "241/241 [==============================] - 408s 2s/step - loss: 0.4539 - binary_accuracy: 0.7553 - binary_io_u: 0.5701 - val_loss: 0.3882 - val_binary_accuracy: 0.8011 - val_binary_io_u: 0.5429\n",
            "Epoch 2/20\n",
            "241/241 [==============================] - 390s 2s/step - loss: 0.3750 - binary_accuracy: 0.8105 - binary_io_u: 0.6577 - val_loss: 0.4101 - val_binary_accuracy: 0.7983 - val_binary_io_u: 0.5122\n",
            "Epoch 3/20\n",
            "241/241 [==============================] - 391s 2s/step - loss: 0.3240 - binary_accuracy: 0.8425 - binary_io_u: 0.7108 - val_loss: 0.5851 - val_binary_accuracy: 0.7635 - val_binary_io_u: 0.5520\n",
            "Epoch 4/20\n",
            "241/241 [==============================] - 391s 2s/step - loss: 0.2809 - binary_accuracy: 0.8674 - binary_io_u: 0.7522 - val_loss: 0.5713 - val_binary_accuracy: 0.7952 - val_binary_io_u: 0.5438\n"
          ]
        }
      ],
      "source": [
        "history = siamese_model.fit(train_dataset,\n",
        "                            steps_per_epoch = train_size//batch_size + 1,\n",
        "                            epochs = 20,\n",
        "                            validation_data=val_dataset,\n",
        "                            validation_steps = val_size//batch_size + 1,\n",
        "                            callbacks = [early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NiVSPGoDTSrx"
      },
      "outputs": [],
      "source": [
        "siamese_model.save('/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/siamese_with_features.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log-loss on Kaggle test set:- Private: 0.42957 Public: 0.43021\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bggl3EASv4kI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnJYBMMoS_FP"
      },
      "source": [
        "Further improvements could be brought by:-\n",
        "\n",
        "1.   Use glove embeddings instead of training new embeddings\n",
        "2.   Experiment with the d_model\n",
        "3.   Using BERT models or more complex models"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2364VXYMkaxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replacing embedding layer with Glove embeddigs"
      ],
      "metadata": {
        "id": "6zzo75upt04g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv99YA0GVYFh"
      },
      "outputs": [],
      "source": [
        "# voc = vectorize_layer.get_vocabulary()\n",
        "# word_index = dict(zip(voc, range(len(voc))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEebLd_tRLzl"
      },
      "outputs": [],
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.840B.300d.zip -P \"/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/\"\n",
        "# !unzip -q \"/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/glove.840B.300d.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us_pkV8GRM0u"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "# path_to_glove_file = os.path.join(\"/content/glove.840B.300d.txt\")\n",
        "\n",
        "# embeddings_index = {}\n",
        "# f = open(path_to_glove_file, encoding='utf-8')\n",
        "\n",
        "# # for line in tqdm(f):\n",
        "# for line in f:\n",
        "#     values = line.split()\n",
        "#     # word = values[0]\n",
        "#     word = (''.join(values[:-300])).lower()   \n",
        "#     # coefs = np.asarray(values[1:], dtype='float32')\n",
        "#     coefs = np.asarray(values[-300:], dtype='float32')\n",
        "#     if(word in embeddings_index):\n",
        "#       embeddings_index[word].append(coefs)\n",
        "#     else:\n",
        "#       embeddings_index[word] = [coefs]\n",
        "\n",
        "\n",
        "# f.close()\n",
        "\n",
        "# print(\"Found %s word vectors.\" % len(embeddings_index))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRkOyi6qvHJZ"
      },
      "outputs": [],
      "source": [
        "# for word in embeddings_index:\n",
        "#   embeddings_index[word] = np.mean(embeddings_index[word],axis = 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KCRo6TlJRMb9"
      },
      "outputs": [],
      "source": [
        "# num_tokens = len(voc) + 2\n",
        "# embedding_dim = 300\n",
        "# hits = 0\n",
        "# misses = 0\n",
        "# missed_words = []\n",
        "# # Prepare embedding matrix\n",
        "# embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "# for word, i in word_index.items():\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "#     if embedding_vector is not None:\n",
        "#         # Words not found in embedding index will be all-zeros.\n",
        "#         # This includes the representation for \"padding\" and \"OOV\"\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "#         hits += 1\n",
        "#     else:\n",
        "#         missed_words.append(word)\n",
        "#         misses += 1\n",
        "# print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating embedding layer"
      ],
      "metadata": {
        "id": "hRaFtxcUudEY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1qQ8YSPiEDC"
      },
      "outputs": [],
      "source": [
        "# embedding_layer = layers.Embedding(\n",
        "#     num_tokens,\n",
        "#     embedding_dim,\n",
        "#     embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "#     trainable=False,\n",
        "#     name = 'embedding_layer_300'\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M77v0vMwxXip"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# pickle.dump({'config': embedding_layer.get_config(),\n",
        "#              'weights': embedding_layer.get_weights()}\n",
        "#             , open(\"/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/embedding_layer.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GxQb4vdsxs5r"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from_disk = pickle.load(open(\"/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/embedding_layer.pkl\", \"rb\"))\n",
        "embedding_layer = layers.Embedding.from_config(from_disk['config'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "76tMAW2diU5-"
      },
      "outputs": [],
      "source": [
        "def build_siamese_network_glove(vocab_size = VOCAB_SIZE,d_model = D_MODEL,dropout_rate = 0.20,batch_size = BATCH_SIZE,max_length = MAX_LENGTH): \n",
        "\n",
        "  def build_tf_lstm_model_glove():\n",
        "    encoded_question = layers.Input(batch_shape = (batch_size,max_length),name = 'input_encoded_question')\n",
        "    embeddings = embedding_layer(encoded_question)\n",
        "    layer_1 =  layers.Bidirectional(layers.LSTM(d_model,activation = 'tanh',return_sequences=True,dropout = dropout_rate,stateful  = True,name = 'lstm_1'),name = 'bidirectional_1')(embeddings)\n",
        "    layer_2 =  layers.Bidirectional(layers.LSTM(d_model,return_sequences=False,stateful  = True,name = 'lstm_2'),name = 'bidirectional_2')(layer_1)\n",
        "    lstm_model  = Model(inputs = [encoded_question],outputs = [layer_2],name = 'LSTM')\n",
        "\n",
        "    return lstm_model\n",
        "\n",
        "\n",
        "  \n",
        "  encoded_question1 = layers.Input(batch_shape = (batch_size,max_length),name = 'input_encoded_question1')\n",
        "  encoded_question2 = layers.Input(batch_shape = (batch_size,max_length),name = 'input_encoded_question2')\n",
        "  \n",
        "  \n",
        "  \n",
        "  lstm_model = build_tf_lstm_model_glove()\n",
        "  \n",
        "  embedded_questions1 = lstm_model(encoded_question1)\n",
        "  embedded_questions2 = lstm_model(encoded_question2)\n",
        "\n",
        "  lengths = layers.Input(batch_shape = (batch_size,3),name = 'input_lengths')\n",
        "  lengths = layers.BatchNormalization(name = 'batch_norm_lengths')(lengths)\n",
        "\n",
        "  lengths_hidden = layers.Dense(d_model//2,name = 'lengths_hidden1')(lengths)\n",
        "  lengths_hidden = layers.BatchNormalization()(lengths_hidden)\n",
        "  lengths_hidden = layers.Activation('relu')(lengths_hidden)\n",
        "  lengths_hidden = layers.Dropout(dropout_rate)(lengths_hidden)\n",
        "\n",
        "  lengths_hidden = layers.Dense(d_model,name = 'lengths_hidden2')(lengths_hidden)\n",
        "\n",
        "  concatenated = tf.concat([embedded_questions1,embedded_questions2,lengths_hidden],axis = -1)\n",
        "  concatenated = layers.BatchNormalization()(concatenated)\n",
        "  concatenated = layers.Activation('relu')(concatenated)\n",
        "  concatenated = layers.Dropout(0.20)(concatenated)\n",
        "\n",
        "  concatenated_dense = layers.Dense(d_model,name = 'concatenated_dense1')(concatenated)\n",
        "  concatenated_dense = layers.BatchNormalization()(concatenated_dense)\n",
        "  concatenated_dense = layers.Activation('relu')(concatenated_dense)\n",
        "  concatenated_dense = layers.Dropout(0.20)(concatenated_dense)\n",
        "\n",
        "  concatenated_dense = layers.Dense(d_model//2,name = 'concatenated_dense2')(concatenated_dense)\n",
        "  concatenated_dense = layers.BatchNormalization()(concatenated_dense)\n",
        "  concatenated_dense = layers.Activation('relu')(concatenated_dense)\n",
        "  concatenated_dense = layers.Dropout(0.20)(concatenated_dense)\n",
        "\n",
        "  \n",
        "  output = layers.Dense(1,name = 'output_logit')(concatenated_dense)\n",
        "\n",
        "  NN_model = Model(inputs = [embedded_questions1,embedded_questions2,lengths_hidden],outputs = [output])\n",
        "\n",
        "  output1 = NN_model((embedded_questions1,embedded_questions2,lengths_hidden))\n",
        "  output2 = NN_model((embedded_questions2,embedded_questions1,lengths_hidden))\n",
        "\n",
        "  output_logit = (output1 + output2)/2\n",
        "\n",
        "  model  = Model(inputs = [encoded_question1,encoded_question2,lengths],outputs = [output_logit])\n",
        "  \n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "V0gEHK7UiqWt"
      },
      "outputs": [],
      "source": [
        "batch_size = BATCH_SIZE\n",
        "siamese_model = build_siamese_network_glove(batch_size = batch_size)\n",
        "\n",
        "siamese_model.compile(loss  =  tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
        "                optimizer='nadam',\n",
        "                metrics = [tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.BinaryIoU()])\n",
        "\n",
        "early_stopping = EarlyStopping(min_delta = 0.01,patience = 5,restore_best_weights=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6WYw2oIygkJ",
        "outputId": "7c4fe922-0782-4ff8-e1f8-0d8d3cba2171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_12 (InputLayer)          [(1536, 3)]          0           []                               \n",
            "                                                                                                  \n",
            " lengths_hidden1 (Dense)        (1536, 150)          600         ['input_12[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (1536, 150)         600         ['lengths_hidden1[1][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (1536, 150)          0           ['batch_normalization_8[1][0]']  \n",
            "                                                                                                  \n",
            " input_encoded_question1 (Input  [(1536, 64)]        0           []                               \n",
            " Layer)                                                                                           \n",
            "                                                                                                  \n",
            " input_encoded_question2 (Input  [(1536, 64)]        0           []                               \n",
            " Layer)                                                                                           \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (1536, 150)          0           ['activation_7[1][0]']           \n",
            "                                                                                                  \n",
            " LSTM (Functional)              (1536, 600)          39754200    ['input_encoded_question1[0][0]',\n",
            "                                                                  'input_encoded_question2[0][0]']\n",
            "                                                                                                  \n",
            " lengths_hidden2 (Dense)        (1536, 300)          45300       ['dropout_8[1][0]']              \n",
            "                                                                                                  \n",
            " model_4 (Functional)           (1536, 1)            503401      ['LSTM[2][0]',                   \n",
            "                                                                  'LSTM[3][0]',                   \n",
            "                                                                  'lengths_hidden2[1][0]',        \n",
            "                                                                  'LSTM[3][0]',                   \n",
            "                                                                  'LSTM[2][0]',                   \n",
            "                                                                  'lengths_hidden2[1][0]']        \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (1536, 1)           0           ['model_4[2][0]',                \n",
            " mbda)                                                            'model_4[3][0]']                \n",
            "                                                                                                  \n",
            " tf.math.truediv_2 (TFOpLambda)  (1536, 1)           0           ['tf.__operators__.add_2[1][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 40,304,101\n",
            "Trainable params: 4,150,501\n",
            "Non-trainable params: 36,153,600\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "siamese_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHcVQIZ9yeCS",
        "outputId": "120c6542-c0ca-4b9b-bdf8-d07ad251130f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "241/241 [==============================] - 397s 2s/step - loss: 0.5042 - binary_accuracy: 0.7190 - binary_io_u_2: 0.5194 - val_loss: 0.4228 - val_binary_accuracy: 0.7971 - val_binary_io_u_2: 0.4966\n",
            "Epoch 2/20\n",
            "241/241 [==============================] - 379s 2s/step - loss: 0.4370 - binary_accuracy: 0.7639 - binary_io_u_2: 0.5838 - val_loss: 0.3601 - val_binary_accuracy: 0.8053 - val_binary_io_u_2: 0.5020\n",
            "Epoch 3/20\n",
            "241/241 [==============================] - 378s 2s/step - loss: 0.4091 - binary_accuracy: 0.7846 - binary_io_u_2: 0.6162 - val_loss: 0.3602 - val_binary_accuracy: 0.7939 - val_binary_io_u_2: 0.4541\n",
            "Epoch 4/20\n",
            "241/241 [==============================] - 377s 2s/step - loss: 0.3880 - binary_accuracy: 0.7986 - binary_io_u_2: 0.6383 - val_loss: 0.3578 - val_binary_accuracy: 0.8143 - val_binary_io_u_2: 0.5559\n",
            "Epoch 5/20\n",
            "241/241 [==============================] - 379s 2s/step - loss: 0.3677 - binary_accuracy: 0.8125 - binary_io_u_2: 0.6602 - val_loss: 0.3632 - val_binary_accuracy: 0.8172 - val_binary_io_u_2: 0.5603\n",
            "Epoch 6/20\n",
            "241/241 [==============================] - 378s 2s/step - loss: 0.3486 - binary_accuracy: 0.8254 - binary_io_u_2: 0.6808 - val_loss: 0.3713 - val_binary_accuracy: 0.8160 - val_binary_io_u_2: 0.5661\n",
            "Epoch 7/20\n",
            "241/241 [==============================] - 379s 2s/step - loss: 0.3288 - binary_accuracy: 0.8384 - binary_io_u_2: 0.7019 - val_loss: 0.3687 - val_binary_accuracy: 0.8203 - val_binary_io_u_2: 0.5820\n",
            "Epoch 8/20\n",
            "241/241 [==============================] - 377s 2s/step - loss: 0.3094 - binary_accuracy: 0.8497 - binary_io_u_2: 0.7206 - val_loss: 0.3801 - val_binary_accuracy: 0.8049 - val_binary_io_u_2: 0.5126\n",
            "Epoch 9/20\n",
            "241/241 [==============================] - 378s 2s/step - loss: 0.2895 - binary_accuracy: 0.8614 - binary_io_u_2: 0.7401 - val_loss: 0.4041 - val_binary_accuracy: 0.8126 - val_binary_io_u_2: 0.5947\n",
            "Epoch 10/20\n",
            "241/241 [==============================] - 378s 2s/step - loss: 0.2706 - binary_accuracy: 0.8722 - binary_io_u_2: 0.7585 - val_loss: 0.4172 - val_binary_accuracy: 0.8147 - val_binary_io_u_2: 0.5793\n",
            "Epoch 11/20\n",
            "241/241 [==============================] - 377s 2s/step - loss: 0.2523 - binary_accuracy: 0.8821 - binary_io_u_2: 0.7754 - val_loss: 0.5261 - val_binary_accuracy: 0.7827 - val_binary_io_u_2: 0.5727\n",
            "Epoch 12/20\n",
            "241/241 [==============================] - 378s 2s/step - loss: 0.2342 - binary_accuracy: 0.8924 - binary_io_u_2: 0.7934 - val_loss: 0.4202 - val_binary_accuracy: 0.8157 - val_binary_io_u_2: 0.5713\n"
          ]
        }
      ],
      "source": [
        "history = siamese_model.fit(train_dataset,\n",
        "                            batch_size = BATCH_SIZE,\n",
        "                            steps_per_epoch = train_size//batch_size + 1,\n",
        "                            epochs = 20,\n",
        "                            validation_data=val_dataset,\n",
        "                            validation_steps = val_size//batch_size + 1,\n",
        "                            callbacks = [early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZYusv8P2zPZs"
      },
      "outputs": [],
      "source": [
        "siamese_model.save('/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/siamese_with_features_and_glove.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log-loss on Kaggle test set:- Private: 0.36949 Public: 0.36655\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YyloRuAnvfQF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWgFM_UJiEhk"
      },
      "source": [
        "### Prediction on test data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction using model without glove embeddings"
      ],
      "metadata": {
        "id": "xeL1lMLUwUNQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SZW2BX_xY37"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024\n",
        "siamese_model = build_siamese_network_glove(batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd9H2OnAxY38"
      },
      "outputs": [],
      "source": [
        "siamese_model.load_weights('/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/siamese_with_features.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61915d0f-7a90-4f72-b218-1a7c9ee33fa1",
        "id": "8sAAhGXmxY38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1528/1528 [==============================] - 738s 482ms/step\n"
          ]
        }
      ],
      "source": [
        "y_test_predict = siamese_model.predict(test_dataset,steps=test_data.shape[0]//batch_size + 1,verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttGzPg_qxY38"
      },
      "outputs": [],
      "source": [
        "test_data['is_duplicate'] = y_test_predict[:test_data.shape[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoPG_3Y5xY38"
      },
      "outputs": [],
      "source": [
        "test_data['is_duplicate'] = test_data['is_duplicate'].apply(lambda x : np.exp(x)/(np.exp(x) + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATGYbj8fxY39"
      },
      "outputs": [],
      "source": [
        "submission = test_data[['test_id','is_duplicate']].copy()\n",
        "submission.to_csv('/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/submission_with_features_.csv',index = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oukGAnjoxZ9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction using model with glove embeddings"
      ],
      "metadata": {
        "id": "Adxz5Xiaxacy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwFDSHVTw9Dv"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024\n",
        "siamese_model = build_siamese_network_glove(batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSWtm63Sw9Dw"
      },
      "outputs": [],
      "source": [
        "siamese_model.load_weights('/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/siamese_with_features_and_glove.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61915d0f-7a90-4f72-b218-1a7c9ee33fa1",
        "id": "eJ53RTMVw9Dx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1528/1528 [==============================] - 738s 482ms/step\n"
          ]
        }
      ],
      "source": [
        "y_test_predict = siamese_model.predict(test_dataset,steps=test_data.shape[0]//batch_size + 1,verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLg2F9Wlw9Dy"
      },
      "outputs": [],
      "source": [
        "test_data['is_duplicate'] = y_test_predict[:test_data.shape[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5kaGgZGw9Dz"
      },
      "outputs": [],
      "source": [
        "test_data['is_duplicate'] = test_data['is_duplicate'].apply(lambda x : np.exp(x)/(np.exp(x) + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z7v2MSXw9Dz"
      },
      "outputs": [],
      "source": [
        "submission = test_data[['test_id','is_duplicate']].copy()\n",
        "submission.to_csv('/content/drive/MyDrive/Machine_Learning/NLP/Text Similarity/quora-questions/siamese_with_features_and_glove.csv',index = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b-ZFbnVrxJx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xWgFM_UJiEhk"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}